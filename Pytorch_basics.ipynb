{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = torch.tensor(np.random.randn(4,4),device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5769, -0.1962,  1.4789,  0.9177],\n",
       "        [ 0.6475,  1.9966,  1.1973,  2.5206],\n",
       "        [ 1.9048,  0.5793,  1.8409,  0.8641],\n",
       "        [ 1.2287,  2.1695, -1.3196,  0.6289]], device='cuda:0',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.5267,  0.9457, -1.2000, -0.7924],\n",
       "        [-0.0651,  3.6593, -4.1359,  1.8847],\n",
       "        [-1.3446, -1.4650,  0.5703,  0.1533],\n",
       "        [-3.7522,  1.9830, -1.5520,  3.1439]], device='cuda:0',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sample+1).mm(sample)#matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.2294, 0.0000],\n",
       "        [0.0000, 0.9931, 0.0389, 2.3124],\n",
       "        [0.8186, 0.0000, 0.7072, 0.0000],\n",
       "        [0.0523, 1.3677, 0.0000, 0.0000]], device='cuda:0',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample*sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = F.relu(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.4789, 0.0000],\n",
       "        [0.0000, 0.9966, 0.1973, 1.5206],\n",
       "        [0.9048, 0.0000, 0.8409, 0.0000],\n",
       "        [0.2287, 1.1695, 0.0000, 0.0000]], device='cuda:0',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.nn.Module API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerFC(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # forward always defines connectivity\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        scores = self.fc2(F.relu(self.fc1(x)))\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "input_size = 50\n",
    "x = torch.zeros((64, input_size))  # minibatch size 64, feature dimension 50\n",
    "model = TwoLayerFC(input_size, 42, 10)\n",
    "scores = model(x)\n",
    "print(scores.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerConvNet(nn.Module):\n",
    "    def __init__(self, in_channel, channel_1, channel_2,fcl_1, num_classes):\n",
    "        super().__init__()\n",
    "        self.bn0 = nn.BatchNorm2d(in_channel,affine=False)\n",
    "        self.conv_1 = nn.Conv2d(in_channel, channel_1, 5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(channel_1)\n",
    "        self.conv_2 = nn.Conv2d(channel_1, channel_2, 3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d((2,2))\n",
    "        self.fc1 = nn.Linear(channel_2*16*16, fcl_1)\n",
    "        self.bn2 = nn.BatchNorm1d(fcl_1)\n",
    "        self.drop = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(fcl_1,num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv_1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv_2(x.clamp(min=0))\n",
    "        x = self.pool1(x)\n",
    "        x = x.view(x.shape[0],-1)#remember to flatten before joining a conv layer and fc layer\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.drop(x)\n",
    "        scores = self.fc2(x.clamp(min=0))\n",
    "        #scores = F.relu(self.fc2(x))\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros((64, 3, 32, 32))  # minibatch size 64, image size [3, 32, 32]\n",
    "model = ThreeLayerConvNet(in_channel=3, channel_1=12, channel_2=8,fcl_1=256, num_classes=10)\n",
    "scores = model(x)\n",
    "print(scores.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_convnet(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/170498071 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to .datasets/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170500096it [00:02, 72623916.88it/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .datasets/cifar-10-python.tar.gz to .datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/170498071 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to datasets/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170500096it [00:02, 78939983.96it/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/cifar-10-python.tar.gz to datasets\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "NUM_TRAIN = 49000\n",
    "\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "\n",
    "cifar10_train = dset.CIFAR10('.datasets', train=True, download=True,\n",
    "                             transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('datasets', train=True, download=True,\n",
    "                           transform=transform)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('datasets', train=False, download=True, \n",
    "                            transform=transform)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write required functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "print_every=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, optimizer, epochs=1):\n",
    "   \n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                check_accuracy(loader_val, model)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 3.3018\n",
      "Checking accuracy on validation set\n",
      "Got 136 / 1000 correct (13.60)\n",
      "\n",
      "Iteration 100, loss = 2.0659\n",
      "Checking accuracy on validation set\n",
      "Got 353 / 1000 correct (35.30)\n",
      "\n",
      "Iteration 200, loss = 1.9727\n",
      "Checking accuracy on validation set\n",
      "Got 374 / 1000 correct (37.40)\n",
      "\n",
      "Iteration 300, loss = 1.8030\n",
      "Checking accuracy on validation set\n",
      "Got 367 / 1000 correct (36.70)\n",
      "\n",
      "Iteration 400, loss = 1.5971\n",
      "Checking accuracy on validation set\n",
      "Got 414 / 1000 correct (41.40)\n",
      "\n",
      "Iteration 500, loss = 2.1596\n",
      "Checking accuracy on validation set\n",
      "Got 413 / 1000 correct (41.30)\n",
      "\n",
      "Iteration 600, loss = 1.6576\n",
      "Checking accuracy on validation set\n",
      "Got 440 / 1000 correct (44.00)\n",
      "\n",
      "Iteration 700, loss = 1.7696\n",
      "Checking accuracy on validation set\n",
      "Got 395 / 1000 correct (39.50)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_size = 4000\n",
    "learning_rate = 1e-2\n",
    "model = TwoLayerFC(3 * 32 * 32, hidden_layer_size, 10)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "trainer(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 2.4112\n",
      "Checking accuracy on validation set\n",
      "Got 124 / 1000 correct (12.40)\n",
      "\n",
      "Iteration 100, loss = 1.7863\n",
      "Checking accuracy on validation set\n",
      "Got 427 / 1000 correct (42.70)\n",
      "\n",
      "Iteration 200, loss = 1.7568\n",
      "Checking accuracy on validation set\n",
      "Got 475 / 1000 correct (47.50)\n",
      "\n",
      "Iteration 300, loss = 1.4410\n",
      "Checking accuracy on validation set\n",
      "Got 494 / 1000 correct (49.40)\n",
      "\n",
      "Iteration 400, loss = 1.4275\n",
      "Checking accuracy on validation set\n",
      "Got 513 / 1000 correct (51.30)\n",
      "\n",
      "Iteration 500, loss = 1.3255\n",
      "Checking accuracy on validation set\n",
      "Got 541 / 1000 correct (54.10)\n",
      "\n",
      "Iteration 600, loss = 1.3190\n",
      "Checking accuracy on validation set\n",
      "Got 537 / 1000 correct (53.70)\n",
      "\n",
      "Iteration 700, loss = 1.1119\n",
      "Checking accuracy on validation set\n",
      "Got 558 / 1000 correct (55.80)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "model = ThreeLayerConvNet(3, 32, 16, 128, 10)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "trainer(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.nn.Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 2.3201\n",
      "Checking accuracy on validation set\n",
      "Got 122 / 1000 correct (12.20)\n",
      "\n",
      "Iteration 100, loss = 1.9144\n",
      "Checking accuracy on validation set\n",
      "Got 339 / 1000 correct (33.90)\n",
      "\n",
      "Iteration 200, loss = 1.8564\n",
      "Checking accuracy on validation set\n",
      "Got 399 / 1000 correct (39.90)\n",
      "\n",
      "Iteration 300, loss = 1.5819\n",
      "Checking accuracy on validation set\n",
      "Got 423 / 1000 correct (42.30)\n",
      "\n",
      "Iteration 400, loss = 1.7657\n",
      "Checking accuracy on validation set\n",
      "Got 406 / 1000 correct (40.60)\n",
      "\n",
      "Iteration 500, loss = 1.7450\n",
      "Checking accuracy on validation set\n",
      "Got 446 / 1000 correct (44.60)\n",
      "\n",
      "Iteration 600, loss = 2.1108\n",
      "Checking accuracy on validation set\n",
      "Got 432 / 1000 correct (43.20)\n",
      "\n",
      "Iteration 700, loss = 1.4147\n",
      "Checking accuracy on validation set\n",
      "Got 420 / 1000 correct (42.00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0],-1)\n",
    "\n",
    "hidden_layer_size = 4000\n",
    "learning_rate = 1e-2\n",
    "\n",
    "model = nn.Sequential(\n",
    "    Flatten(),\n",
    "    nn.Linear(3 * 32 * 32, hidden_layer_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_layer_size, 10),\n",
    ")\n",
    "\n",
    "# you can use Nesterov momentum in optim.SGD\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                     momentum=0.9, nesterov=True)\n",
    "\n",
    "trainer(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 0.5923\n",
      "Checking accuracy on validation set\n",
      "Got 611 / 1000 correct (61.10)\n",
      "\n",
      "Iteration 100, loss = 0.6262\n",
      "Checking accuracy on validation set\n",
      "Got 612 / 1000 correct (61.20)\n",
      "\n",
      "Iteration 200, loss = 0.5149\n",
      "Checking accuracy on validation set\n",
      "Got 618 / 1000 correct (61.80)\n",
      "\n",
      "Iteration 300, loss = 0.8515\n",
      "Checking accuracy on validation set\n",
      "Got 622 / 1000 correct (62.20)\n",
      "\n",
      "Iteration 400, loss = 0.4456\n",
      "Checking accuracy on validation set\n",
      "Got 630 / 1000 correct (63.00)\n",
      "\n",
      "Iteration 500, loss = 0.7495\n",
      "Checking accuracy on validation set\n",
      "Got 630 / 1000 correct (63.00)\n",
      "\n",
      "Iteration 600, loss = 0.5492\n",
      "Checking accuracy on validation set\n",
      "Got 634 / 1000 correct (63.40)\n",
      "\n",
      "Iteration 700, loss = 0.5822\n",
      "Checking accuracy on validation set\n",
      "Got 632 / 1000 correct (63.20)\n",
      "\n",
      "Iteration 0, loss = 0.7787\n",
      "Checking accuracy on validation set\n",
      "Got 634 / 1000 correct (63.40)\n",
      "\n",
      "Iteration 100, loss = 0.9094\n",
      "Checking accuracy on validation set\n",
      "Got 638 / 1000 correct (63.80)\n",
      "\n",
      "Iteration 200, loss = 0.8287\n",
      "Checking accuracy on validation set\n",
      "Got 637 / 1000 correct (63.70)\n",
      "\n",
      "Iteration 300, loss = 0.6380\n",
      "Checking accuracy on validation set\n",
      "Got 634 / 1000 correct (63.40)\n",
      "\n",
      "Iteration 400, loss = 0.3359\n",
      "Checking accuracy on validation set\n",
      "Got 636 / 1000 correct (63.60)\n",
      "\n",
      "Iteration 500, loss = 0.5488\n",
      "Checking accuracy on validation set\n",
      "Got 636 / 1000 correct (63.60)\n",
      "\n",
      "Iteration 600, loss = 0.6375\n",
      "Checking accuracy on validation set\n",
      "Got 635 / 1000 correct (63.50)\n",
      "\n",
      "Iteration 700, loss = 0.5745\n",
      "Checking accuracy on validation set\n",
      "Got 639 / 1000 correct (63.90)\n",
      "\n",
      "Iteration 0, loss = 0.6378\n",
      "Checking accuracy on validation set\n",
      "Got 640 / 1000 correct (64.00)\n",
      "\n",
      "Iteration 100, loss = 0.5594\n",
      "Checking accuracy on validation set\n",
      "Got 638 / 1000 correct (63.80)\n",
      "\n",
      "Iteration 200, loss = 0.4575\n",
      "Checking accuracy on validation set\n",
      "Got 636 / 1000 correct (63.60)\n",
      "\n",
      "Iteration 300, loss = 0.5832\n",
      "Checking accuracy on validation set\n",
      "Got 633 / 1000 correct (63.30)\n",
      "\n",
      "Iteration 400, loss = 0.5252\n",
      "Checking accuracy on validation set\n",
      "Got 637 / 1000 correct (63.70)\n",
      "\n",
      "Iteration 500, loss = 0.5684\n",
      "Checking accuracy on validation set\n",
      "Got 637 / 1000 correct (63.70)\n",
      "\n",
      "Iteration 600, loss = 0.5604\n",
      "Checking accuracy on validation set\n",
      "Got 636 / 1000 correct (63.60)\n",
      "\n",
      "Iteration 700, loss = 0.6612\n",
      "Checking accuracy on validation set\n",
      "Got 642 / 1000 correct (64.20)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "channel_1 = 32\n",
    "channel_2 = 16\n",
    "learning_rate = 1e-4\n",
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "nn.Conv2d(3, 32, 5, padding = 2),\n",
    "nn.ReLU(),\n",
    "nn.Conv2d(32, 16, 3, padding = 1),\n",
    "    Flatten(),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16*32*32, 10)\n",
    ")\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                     momentum=0.9, nesterov=True)\n",
    "\n",
    "trainer(model, optimizer,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
